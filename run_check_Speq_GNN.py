import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
import pandas as pd
from pathlib import Path

from numba.cuda import const
from tqdm import tqdm
from tqdm.contrib import tenumerate
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
from typing import Optional, Tuple, Callable

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ğŸš¨ å®Ÿè¡Œç’°å¢ƒã«å¿œã˜ã¦ãƒ‘ã‚¹ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ ğŸš¨
try:
	# models/graph_utils.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
	from models.graph_utils import GraphConfig, NodeSelectionType, EdgeSelectionType
	# models/check_Speq_GNN.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
	from models.check_SpeqGNN import CheckSpeqGNN
	# CsvDataset.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨ (ä»Šå›ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã«å®šç¾©)

	# mymodule/confirmation_GPU.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
	from mymodule import confirmation_GPU
	# mymodule/my_func.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä¸»ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆç”¨)
	from mymodule import my_func, const
except ImportError as e:
	print(f"ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {e}", file=sys.stderr)
	sys.exit(1)


# --- 1. æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å®šç¾© ---

# CsvSpectralDataset.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€å¿…è¦ãªå…¨ã¦ã®ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¿”ã™ã‚ˆã†ã«æ‹¡å¼µ
class CheckSpectralDataset(Dataset):
	"""
	CheckSpeqGNNãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«å¿…è¦ãªã€ãƒã‚¤ã‚ºã‚ã‚Š/ã‚¯ãƒªãƒ¼ãƒ³ä¸¡æ–¹ã®ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¿”ã™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚
	"""

	def __init__(self, csv_path, input_column_header, sample_rate=16000, max_length_sec=None, n_fft=512, hop_length=256,
	             win_length=None, device='cpu'):
		self.device = device
		self.teacher_column = "clean"
		self.input_column = input_column_header
		self.sample_rate = sample_rate
		self.n_fft = n_fft
		self.hop_length = hop_length
		self.win_length = win_length if win_length is not None else n_fft

		self.max_length_samples = max_length_sec * sample_rate if max_length_sec is not None else None

		# è¤‡ç´ ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ å¤‰æ›å™¨ã®åˆæœŸåŒ–
		self.stft_transform_complex = torchaudio.transforms.Spectrogram(
			n_fft=n_fft, hop_length=hop_length, win_length=self.win_length,
			window_fn=torch.hann_window, power=None, return_complex=True,
		)

		# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
		try:
			self.data_df = pd.read_csv(csv_path)
		except FileNotFoundError:
			raise FileNotFoundError(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

		if self.teacher_column not in self.data_df.columns or self.input_column not in self.data_df.columns:
			raise ValueError(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVã«å¿…è¦ãªåˆ— ('{self.teacher_column}' ã¾ãŸã¯ '{self.input_column}') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

		self.data_df.dropna(subset=[self.teacher_column, self.input_column], inplace=True)
		self.data_df = self.data_df[(self.data_df[self.teacher_column] != "") & (self.data_df[self.input_column] != "")]
		print(f"âœ… [æ¤œè¨¼ç”¨] {csv_path} ã‹ã‚‰ {len(self.data_df)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

	def __getitem__(self, index):
		row = self.data_df.iloc[index]
		clean_path = Path(row[self.teacher_column])
		noisy_path = Path(row[self.input_column])

		# éŸ³å£°æ³¢å½¢ã®èª­ã¿è¾¼ã¿ (ãƒ¢ãƒãƒ©ãƒ«ã‚’æƒ³å®š)
		clean_waveform, _ = torchaudio.load(clean_path)
		noisy_waveform, _ = torchaudio.load(noisy_path)

		# è¤‡æ•°ãƒãƒ£ãƒ³ãƒãƒ«ã®å ´åˆã¯æœ€åˆã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é¸æŠ
		if noisy_waveform.shape[0] > 1: noisy_waveform = noisy_waveform[0].unsqueeze(0)
		if clean_waveform.shape[0] > 1: clean_waveform = clean_waveform[0].unsqueeze(0)

		# æ³¢å½¢é•·ã®èª¿æ•´
		min_len = min(noisy_waveform.shape[-1], clean_waveform.shape[-1])
		if self.max_length_samples is not None:
			min_len = min(min_len, self.max_length_samples)

		noisy_waveform = noisy_waveform[:, :min_len]
		clean_waveform = clean_waveform[:, :min_len]
		noisy_length = noisy_waveform.shape[-1]

		# STFTã®ãŸã‚ã«ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒã‚’å‰Šé™¤ [1, L] -> [L]
		noisy_waveform_squeezed = noisy_waveform.squeeze(0)
		clean_waveform_squeezed = clean_waveform.squeeze(0)

		# STFTé©ç”¨ (CPUä¸Šã§å®Ÿè¡Œ)
		noisy_complex_spec = self.stft_transform_complex(noisy_waveform_squeezed)
		clean_complex_spec = self.stft_transform_complex(clean_waveform_squeezed)

		# æŒ¯å¹…ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¨ˆç®—ã—ã€ãƒãƒ£ãƒãƒ«æ¬¡å…ƒ(1)ã®ã¿ã‚’è¿½åŠ  (F, T -> 1, F, T)
		# ä¸è¦ãª .unsqueeze(0) ã‚’å‰Šé™¤ã—ã€ãƒãƒ£ãƒãƒ«æ¬¡å…ƒã®æŒ¿å…¥ã‚’ä¸€åº¦ã ã‘ã«ã—ã¾ã™ã€‚

		# ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰:
		noisy_magnitude_spec = torch.abs(noisy_complex_spec).unsqueeze(0)  # [1, F, T]
		clean_magnitude_spec = torch.abs(clean_complex_spec).unsqueeze(0)  # [1, F, T]

		noisy_length = noisy_waveform.shape[-1]
		clean_length = clean_waveform.shape[-1]

		file_name = clean_path.stem

		# ãƒã‚¤ã‚ºã‚ã‚ŠæŒ¯å¹…[1, F, T]ã€ã‚¯ãƒªãƒ¼ãƒ³æŒ¯å¹…[1, F, T]ã€ãƒã‚¤ã‚ºã‚ã‚Šè¤‡ç´ [F, T]ã€å…ƒã®é•·ã•[int]ã€ãƒ•ã‚¡ã‚¤ãƒ«å[str]ã‚’è¿”ã™
		return noisy_magnitude_spec, clean_magnitude_spec, noisy_complex_spec, clean_complex_spec, noisy_length, clean_length, file_name

	def __len__(self):
		return len(self.data_df)


# --- 2. å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æœ¬ä½“ ---

def run_analysis(
		model_path: str,
		test_csv_path: str,
		output_dir: str,
		gnn_type: str,
		num_node: int,
		max_length_sec: Optional[int],
		stft_params: dict,
		csv_input_column: str,
):
	device = confirmation_GPU.get_device()
	print(f"åˆ†æã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹: {device}")

	# ãƒ¢ãƒ‡ãƒ«è¨­å®š
	graph_config = GraphConfig(
		num_edges=num_node,
		node_selection=NodeSelectionType.ALL,
		edge_selection=EdgeSelectionType.KNN,
		bidirectional=True,
	)

	# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
	model = CheckSpeqGNN(
		n_channels=1,
		n_classes=1,
		gnn_type=gnn_type,
		graph_config=graph_config,
		**stft_params,
	).to(device)

	# å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
	try:
		model_name = Path(model_path).stem
		# ä¸€èˆ¬çš„ãªã€Œæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã€ã®å‘½åè¦å‰‡ã‚’å„ªå…ˆã—ã¦ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
		best_model_path = Path(model_path).parent / f"BEST_{model_name}.pth"
		loaded_state_dict = torch.load(best_model_path, map_location=device)

		# å†—é•·ãªã‚­ãƒ¼åï¼ˆä¾‹: 'module.' ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰ã®å‰Šé™¤
		if list(loaded_state_dict.keys())[0].startswith('module.'):
			loaded_state_dict = {k.replace('module.', ''): v for k, v in loaded_state_dict.items()}

		model.load_state_dict(loaded_state_dict)
		print(f"âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ {best_model_path.name} ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
	except Exception as e:
		print(f"âš ï¸ è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã§ç¶šè¡Œã—ã¾ã™ã€‚")

	# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
	dataloader = DataLoader(
		CheckSpectralDataset(
			csv_path=test_csv_path,
			input_column_header=csv_input_column,
			max_length_sec=max_length_sec,
			**stft_params,
			device=device,
		),
		batch_size=1,
		shuffle=False
	)

	# --- ãƒ‡ãƒ¼ã‚¿åé›†ã®å®Ÿè¡Œ ---
	model.eval()
	all_node_losses = []
	node_connection_counts = defaultdict(int)
	num_nodes_per_file = None
	total_files = 0

	# U-Netã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¿‚æ•°
	downsample_factor = 2 ** 3
	# ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã®å‘¨æ³¢æ•°ãƒ“ãƒ³æ•°
	estimated_freq_bins_bottleneck = int(np.ceil((stft_params['n_fft'] // 2 + 1) / downsample_factor))

	with torch.no_grad():
		print("ãƒãƒ¼ãƒ‰èª¤å·®ã¨ã‚¨ãƒƒã‚¸æ¥ç¶šå›æ•°ã®åé›†ã‚’é–‹å§‹...")
		# tqdm(dataloader, ...) ã®ä»£ã‚ã‚Šã«enumerateã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿åé›†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–
		for i, batch in tenumerate(dataloader):
			# print(batch)
			# print(len(batch))
			# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æ›´æ–°
			if i % 50 == 0 or i == len(dataloader) - 1:
				tqdm.write(f"Collecting Node Metrics: {i}/{len(dataloader)}")

			# --- 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®è¦ç´ ã‚’æ˜ç¤ºçš„ã«ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ï¼ˆ5è¦ç´ ã‚’å‰æï¼‰ ---
			# 5ã¤ã®è¦ç´ ã‚’æ˜ç¤ºçš„ã«ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
			noisy_mag, clean_mag, noisy_complex, clean_complex, noisy_length, clean_length, file_name = batch

			# Tensorã‚’Pythonã®intã«å¤‰æ›
			noisy_length_int = noisy_length.item()
			clean_length_int = clean_length.item()
			# ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã‹ã‚‰æ–‡å­—åˆ—ã‚’å–å¾—
			file_name_str = file_name[0] if isinstance(file_name, list) else file_name

			# CUDAã«ç§»å‹•
			noisy_mag = noisy_mag.to(device)
			clean_mag = clean_mag.to(device)
			noisy_complex = noisy_complex.to(device)
			clean_complex = clean_complex.to(device)

			# --- 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®å®Ÿè¡Œ ---
			# ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ã€‚å¼•æ•°ã¯4ã¤ã§ã€original_lengthã¯intã«å¤‰æ›æ¸ˆã¿
			_, noisy_node, noisy_index = model(noisy_mag, noisy_complex, noisy_length_int)
			_, clean_node, clean_index = model(clean_mag, clean_complex, clean_length_int)

			# --- 3. Excelã«å‡ºåŠ›
			output_path = f"{output_dir}/{file_name_str}.xlsx"
			my_func.make_dir(os.path.dirname(output_path))
			# Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãå‡ºã—
			with pd.ExcelWriter(output_path) as writer:
				pd.DataFrame(noisy_node.cpu().numpy()).to_excel(writer, sheet_name="node", startcol=0)
				pd.DataFrame(clean_node.cpu().numpy()).to_excel(writer, sheet_name="node", startcol=noisy_node.shape[1] + 1)
				pd.DataFrame(noisy_index.cpu().numpy().T).to_excel(writer, sheet_name="noisy_index")
				pd.DataFrame(clean_index.cpu().numpy().T).to_excel(writer, sheet_name="clean_index")



	print(f"\n=======================================================")
	print(f"âœ… GNNãƒãƒ¼ãƒ‰åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
	print(f"=======================================================")


# --- 3. å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---

if __name__ == "__main__":
	# ğŸš¨ğŸš¨ğŸš¨ ä»¥ä¸‹ã‚’**å¿…ãš**ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ ğŸš¨ğŸš¨ğŸš¨

	# 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
	# ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«å
	model = "SpeqGAT"
	wave_type = "noise_reverb"
	speech_type = "DEMAND_DEMAND"
	MODEL_BASE_DIR = f"{const.PTH_DIR}/{speech_type}/{model}"  # ä¾‹: "models/saved_models/SpeqGAT_noise_only"
	MODEL_NAME = f"{model}_{wave_type}"
	MODEL_PATH = f"{MODEL_BASE_DIR}/SISDR_SpeqGAT_{wave_type}_32node_all_knn.pth"  # ä¾‹: BEST_SpeqGAT_noise_only.pthã‚’ãƒ­ãƒ¼ãƒ‰
	# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
	CSV_PATH = Path(f"{const.MIX_DATA_DIR}/{speech_type}/test.csv")
	# 3. å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
	OUTPUT_DIR = f"{const.OUTPUT_WAV_DIR}/{speech_type}/{model}/gnn_node_analysis"

	# 4. ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (å­¦ç¿’æ™‚ã¨ä¸€è‡´ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)
	GNN_TYPE = "GAT"
	NUM_NODE_EDGES = 32
	MAX_LENGTH_SEC = None
	CSV_INPUT_COL = "noise_reverb"

	STFT_PARAMS = {
		"n_fft": 512,
		"hop_length": 256,
		"win_length": 512,
	}

	print("--- GNNãƒãƒ¼ãƒ‰åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ ---")

	# å®Ÿè¡Œ
	run_analysis(
		model_path=Path(MODEL_PATH),
		test_csv_path=str(CSV_PATH),
		output_dir=OUTPUT_DIR,
		gnn_type=GNN_TYPE,
		num_node=NUM_NODE_EDGES,
		max_length_sec=MAX_LENGTH_SEC,
		stft_params=STFT_PARAMS,
		csv_input_column=CSV_INPUT_COL,
	)
