# coding:utf-8
import sys
import torch
import h5py # â˜… è¿½åŠ 
from typing import Optional
from pathlib import Path
import torchaudio
import pandas as pd

from numba.cuda import const
from tqdm import tqdm
from tqdm.contrib import tenumerate
from torch.utils.data import Dataset, DataLoader

# models/graph_utils.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models.graph_utils import GraphConfig, NodeSelectionType, EdgeSelectionType
# models/check_Speq_GNN.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models.check_SpeqGNN import CheckSpeqGNN
# CsvDataset.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨ (ä»Šå›ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã«å®šç¾©)

# mymodule/confirmation_GPU.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mymodule import confirmation_GPU
# mymodule/my_func.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä¸»ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆç”¨)
from mymodule import my_func, const


# --- 1. æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å®šç¾© ---

# CsvSpectralDataset.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€å¿…è¦ãªå…¨ã¦ã®ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¿”ã™ã‚ˆã†ã«æ‹¡å¼µ
class CheckSpectralDataset(Dataset):
	"""
	CheckSpeqGNNãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«å¿…è¦ãªã€ãƒã‚¤ã‚ºã‚ã‚Š/ã‚¯ãƒªãƒ¼ãƒ³ä¸¡æ–¹ã®ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¿”ã™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚
	"""

	def __init__(self, csv_path, input_column_header, sample_rate=16000, max_length_sec=None, n_fft=512, hop_length=256,
	             win_length=None, device='cpu'):
		self.device = device
		self.teacher_column = "clean"
		self.input_column = input_column_header
		self.sample_rate = sample_rate
		self.n_fft = n_fft
		self.hop_length = hop_length
		self.win_length = win_length if win_length is not None else n_fft

		self.max_length_samples = max_length_sec * sample_rate if max_length_sec is not None else None

		# è¤‡ç´ ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ å¤‰æ›å™¨ã®åˆæœŸåŒ–
		self.stft_transform_complex = torchaudio.transforms.Spectrogram(
			n_fft=n_fft, hop_length=hop_length, win_length=self.win_length,
			window_fn=torch.hann_window, power=None, return_complex=True,
		)

		# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
		try:
			self.data_df = pd.read_csv(csv_path)
		except FileNotFoundError:
			raise FileNotFoundError(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

		if self.teacher_column not in self.data_df.columns or self.input_column not in self.data_df.columns:
			raise ValueError(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVã«å¿…è¦ãªåˆ— ('{self.teacher_column}' ã¾ãŸã¯ '{self.input_column}') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

		self.data_df.dropna(subset=[self.teacher_column, self.input_column], inplace=True)
		self.data_df = self.data_df[(self.data_df[self.teacher_column] != "") & (self.data_df[self.input_column] != "")]
		print(f"âœ… [æ¤œè¨¼ç”¨] {csv_path} ã‹ã‚‰ {len(self.data_df)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

	def __getitem__(self, index):
		row = self.data_df.iloc[index]
		clean_path = Path(row[self.teacher_column])
		noisy_path = Path(row[self.input_column])

		# éŸ³å£°æ³¢å½¢ã®èª­ã¿è¾¼ã¿ (ãƒ¢ãƒãƒ©ãƒ«ã‚’æƒ³å®š)
		clean_waveform, _ = torchaudio.load(clean_path)
		noisy_waveform, _ = torchaudio.load(noisy_path)

		# è¤‡æ•°ãƒãƒ£ãƒ³ãƒãƒ«ã®å ´åˆã¯æœ€åˆã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é¸æŠ
		if noisy_waveform.shape[0] > 1: noisy_waveform = noisy_waveform[0].unsqueeze(0)
		if clean_waveform.shape[0] > 1: clean_waveform = clean_waveform[0].unsqueeze(0)

		# æ³¢å½¢é•·ã®èª¿æ•´
		min_len = min(noisy_waveform.shape[-1], clean_waveform.shape[-1])
		if self.max_length_samples is not None:
			min_len = min(min_len, self.max_length_samples)

		noisy_waveform = noisy_waveform[:, :min_len]
		clean_waveform = clean_waveform[:, :min_len]
		noisy_length = noisy_waveform.shape[-1]

		# STFTã®ãŸã‚ã«ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒã‚’å‰Šé™¤ [1, L] -> [L]
		noisy_waveform_squeezed = noisy_waveform.squeeze(0)
		clean_waveform_squeezed = clean_waveform.squeeze(0)

		# STFTé©ç”¨ (CPUä¸Šã§å®Ÿè¡Œ)
		noisy_complex_spec = self.stft_transform_complex(noisy_waveform_squeezed)
		clean_complex_spec = self.stft_transform_complex(clean_waveform_squeezed)

		# æŒ¯å¹…ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’è¨ˆç®—ã—ã€ãƒãƒ£ãƒãƒ«æ¬¡å…ƒ(1)ã®ã¿ã‚’è¿½åŠ  (F, T -> 1, F, T)
		# ä¸è¦ãª .unsqueeze(0) ã‚’å‰Šé™¤ã—ã€ãƒãƒ£ãƒãƒ«æ¬¡å…ƒã®æŒ¿å…¥ã‚’ä¸€åº¦ã ã‘ã«ã—ã¾ã™ã€‚

		# ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰:
		noisy_magnitude_spec = torch.abs(noisy_complex_spec).unsqueeze(0)  # [1, F, T]
		clean_magnitude_spec = torch.abs(clean_complex_spec).unsqueeze(0)  # [1, F, T]

		noisy_length = noisy_waveform.shape[-1]
		clean_length = clean_waveform.shape[-1]

		file_name = noisy_path.stem

		# ãƒã‚¤ã‚ºã‚ã‚ŠæŒ¯å¹…[1, F, T]ã€ã‚¯ãƒªãƒ¼ãƒ³æŒ¯å¹…[1, F, T]ã€ãƒã‚¤ã‚ºã‚ã‚Šè¤‡ç´ [F, T]ã€å…ƒã®é•·ã•[int]ã€ãƒ•ã‚¡ã‚¤ãƒ«å[str]ã‚’è¿”ã™
		return noisy_magnitude_spec, clean_magnitude_spec, noisy_complex_spec, clean_complex_spec, noisy_length, clean_length, file_name

	def __len__(self):
		return len(self.data_df)


# --- 2. å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æœ¬ä½“ ---

def run_analysis(
		model_path: str,
		test_csv_path: str,
		output_dir: str, # â˜… HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
		hdf5_filename: str, # â˜… å‡ºåŠ›HDF5ãƒ•ã‚¡ã‚¤ãƒ«å
		gnn_type: str,
		num_node: int,
		max_length_sec: Optional[int],
		stft_params: dict,
		csv_input_column: str,
):
	device = confirmation_GPU.get_device()
	print(f"åˆ†æã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹: {device}")

	# ... (ãƒ¢ãƒ‡ãƒ«è¨­å®šã€ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã€é‡ã¿ãƒ­ãƒ¼ãƒ‰ã¯å¤‰æ›´ãªã—) ...
	# ãƒ¢ãƒ‡ãƒ«è¨­å®š
	graph_config = GraphConfig(
		num_edges=num_node,
		node_selection=NodeSelectionType.ALL,
		edge_selection=EdgeSelectionType.KNN,
		bidirectional=True,
	)
	# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
	model = CheckSpeqGNN(
		n_channels=1,
		n_classes=1,
		gnn_type=gnn_type,
		graph_config=graph_config,
		**stft_params,
	).to(device)

	# å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
	try:
		model_name = Path(model_path).stem
		# ä¸€èˆ¬çš„ãªã€Œæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã€ã®å‘½åè¦å‰‡ã‚’å„ªå…ˆã—ã¦ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
		best_model_path = Path(model_path).parent / f"{model_name}.pth"
		loaded_state_dict = torch.load(best_model_path, map_location=device)

		# å†—é•·ãªã‚­ãƒ¼åï¼ˆä¾‹: 'module.' ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰ã®å‰Šé™¤
		if list(loaded_state_dict.keys())[0].startswith('module.'):
			loaded_state_dict = {k.replace('module.', ''): v for k, v in loaded_state_dict.items()}

		model.load_state_dict(loaded_state_dict)
		print(f"âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ {best_model_path.name} ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
	except Exception as e:
		print(f"âš ï¸ è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã§ç¶šè¡Œã—ã¾ã™ã€‚")

	# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ (å¤‰æ›´ãªã—)
	dataloader = DataLoader(
		CheckSpectralDataset(
			csv_path=test_csv_path,
			input_column_header=csv_input_column,
			max_length_sec=max_length_sec,
			**stft_params,
			device=device, # CheckSpectralDataset ã« device å¼•æ•°ãŒãªã„å ´åˆã¯å‰Šé™¤
		),
		batch_size=1, # â˜… ãƒãƒƒãƒã‚µã‚¤ã‚º1ã‚’ç¶­æŒ (HDF5ã¸ã®æ›¸ãè¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ãŒãƒãƒƒãƒ=1å‰æã®ãŸã‚)
		shuffle=False,
		num_workers=4 # â˜… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–ã®ãŸã‚ num_workers ã‚’è¿½åŠ  (ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´)
	)

	# --- HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã ---
	output_hdf5_path = Path(output_dir) / hdf5_filename
	my_func.make_dir(str(output_hdf5_path)) # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
	hdf5_file = h5py.File(output_hdf5_path, 'w')
	print(f"âœ… çµæœã‚’ {output_hdf5_path} ã«å‡ºåŠ›ã—ã¾ã™ã€‚")


	# --- ãƒ‡ãƒ¼ã‚¿åé›†ã®å®Ÿè¡Œ ---
	model.eval()
	# all_node_losses ã‚„ node_connection_counts ã¯ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ä½¿ã‚ã‚Œã¦ã„ãªã„ã‚ˆã†ãªã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
	# all_node_losses = []
	# node_connection_counts = defaultdict(int)
	# num_nodes_per_file = None
	# total_files = 0

	# U-Netã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¿‚æ•° (ã‚‚ã—ä½¿ã†ãªã‚‰æ®‹ã™)
	# downsample_factor = 2 ** 3
	# estimated_freq_bins_bottleneck = int(np.ceil((stft_params['n_fft'] // 2 + 1) / downsample_factor))

	try: # â˜… ãƒ•ã‚¡ã‚¤ãƒ«I/Oã‚¨ãƒ©ãƒ¼ç­‰ã«å‚™ãˆã¦ try...finally ã‚’è¿½åŠ 
		with torch.no_grad():
			print("ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã¨ã‚¨ãƒƒã‚¸æƒ…å ±ã®åé›†ã‚’é–‹å§‹...")
			for i, batch in tenumerate(dataloader):
				if i % 50 == 0 or i == len(dataloader) - 1:
					tqdm.write(f"Processing: {i}/{len(dataloader)}")

				# --- 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®è¦ç´ ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ ---
				noisy_mag, clean_mag, noisy_complex, clean_complex, noisy_length, clean_length, file_name = batch

				noisy_length_int = noisy_length.item()
				clean_length_int = clean_length.item()
				# ãƒ•ã‚¡ã‚¤ãƒ«åã¯ãƒªã‚¹ãƒˆã®å ´åˆãŒã‚ã‚‹ã®ã§æœ€åˆã®è¦ç´ ã‚’å–å¾—
				file_name_str = file_name[0] if isinstance(file_name, (list, tuple)) else file_name
				# print(file_name_str) # ãƒ‡ãƒãƒƒã‚°ç”¨

				# exit(2) # å…ƒã®ã‚³ãƒ¼ãƒ‰ã«ã‚ã£ãŸ exit ã‚’å‰Šé™¤

				# CUDAã«ç§»å‹•
				noisy_mag = noisy_mag.to(device)
				clean_mag = clean_mag.to(device)
				noisy_complex = noisy_complex.to(device)
				clean_complex = clean_complex.to(device)

				# --- 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®å®Ÿè¡Œ ---
				_, noisy_node, noisy_index = model(noisy_mag, noisy_complex, noisy_length_int)
				_, clean_node, clean_index = model(clean_mag, clean_complex, clean_length_int)

				# --- 3. HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ› ---
				# ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã«ã—ãŸã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ (å­˜åœ¨ã™ã‚Œã°ä¸Šæ›¸ã)
				file_group = hdf5_file.create_group(file_name_str)

				# å„ãƒ‡ãƒ¼ã‚¿ã‚’NumPyé…åˆ—ã«å¤‰æ›ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ä¿å­˜
				file_group.create_dataset("noisy_node", data=noisy_node.cpu().numpy())
				file_group.create_dataset("clean_node", data=clean_node.cpu().numpy())
				file_group.create_dataset("error_node", data=(clean_node - noisy_node).cpu().numpy())
				file_group.create_dataset("noisy_index", data=noisy_index.cpu().numpy().T) # å…ƒã®Excelå‡ºåŠ›ã«åˆã‚ã›ã¦è»¢ç½®
				file_group.create_dataset("clean_index", data=clean_index.cpu().numpy().T) # å…ƒã®Excelå‡ºåŠ›ã«åˆã‚ã›ã¦è»¢ç½®

	finally: # â˜… ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
		if 'hdf5_file' in locals() and hdf5_file:
			hdf5_file.close()
			print(f"âœ… HDF5ãƒ•ã‚¡ã‚¤ãƒ« {output_hdf5_path} ã‚’é–‰ã˜ã¾ã—ãŸã€‚")


	print(f"\n=======================================================")
	print(f"âœ… GNNãƒãƒ¼ãƒ‰åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
	print(f"=======================================================")


# --- 3. å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---

if __name__ == "__main__":
	# ğŸš¨ğŸš¨ğŸš¨ ä»¥ä¸‹ã‚’**å¿…ãš**ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ ğŸš¨ğŸš¨ğŸš¨

	# 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
	model = "SpeqGAT"
	wave_type = "noise_reverb"
	speech_type = "DEMAND_DEMAND"
	MODEL_BASE_DIR = f"{const.PTH_DIR}/{speech_type}/{model}"
	MODEL_NAME = f"{model}_{wave_type}"
	# â˜… å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä¿®æ­£
	MODEL_PATH = f"{MODEL_BASE_DIR}/BEST_SISDR_{model}_{wave_type}_32node_all_knn.pth" # BESTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã“ã¨ã‚’æ¨å¥¨

	# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
	CSV_PATH = Path(f"{const.MIX_DATA_DIR}/{speech_type}/test.csv")

	# 3. å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨HDF5ãƒ•ã‚¡ã‚¤ãƒ«å
	OUTPUT_DIR = f"{const.OUTPUT_WAV_DIR}/{speech_type}/{model}/gnn_node_analysis_hdf5/{wave_type}" # â˜… å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤‰æ›´
	HDF5_FILENAME = f"{MODEL_NAME}_analysis_results.h5" # â˜… HDF5ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š

	# 4. ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (å­¦ç¿’æ™‚ã¨ä¸€è‡´ã•ã›ã‚‹)
	GNN_TYPE = "GAT"
	NUM_NODE_EDGES = 32
	MAX_LENGTH_SEC = None
	CSV_INPUT_COL = "noise_reverb" # â˜… CSVå†…ã®å…¥åŠ›åˆ—å

	STFT_PARAMS = {
		"n_fft": 512,
		"hop_length": 256,
		"win_length": 512,
	}

	print("--- GNNãƒãƒ¼ãƒ‰åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ (HDF5å‡ºåŠ›) ---")

	# å®Ÿè¡Œ (å¼•æ•°ã‚’æ›´æ–°)
	run_analysis(
		model_path=Path(MODEL_PATH),
		test_csv_path=str(CSV_PATH),
		output_dir=OUTPUT_DIR,       # â˜…
		hdf5_filename=HDF5_FILENAME, # â˜…
		gnn_type=GNN_TYPE,
		num_node=NUM_NODE_EDGES,
		max_length_sec=MAX_LENGTH_SEC,
		stft_params=STFT_PARAMS,
		csv_input_column=CSV_INPUT_COL,
	)