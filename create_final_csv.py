import csv
import json
import os.path
import sys
from pathlib import Path

from tqdm import tqdm

# ===================================================================
# â–¼â–¼â–¼ è¨­å®šé …ç›® â–¼â–¼â–¼
# ===================================================================

# --- å…¥åŠ›è¨­å®š ---
# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸ˆã¿ã®JSONãƒ•ã‚¡ã‚¤ãƒ«
DEFAULT_JSON_PATH = "sampled_vctk_file_list.json"

# æ‹¡å¼µã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DEFAULT_DATASET_ROOT = Path("C:/Users/kataoka-lab/Desktop/sound_data/mix_data/DEMAND_DEMAND_5dB_500msec")

# --- å‡ºåŠ›è¨­å®š ---
# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼
CSV_HEADER = [
    "clean",
    "noise_only",
    "reverb_only",
    "noise_reverb",
]

# å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã«å¯¾å¿œã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
CONDITION_DIRS = ["clean", "noise_only", "reverbe_only", "noise_reverbe"]

# ===================================================================
# â–²â–²â–² è¨­å®šé …ç›® â–²â–²â–²
# ===================================================================


def create_final_csv_list_prefix(json_path: Path, dataset_root: Path):
    """
    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸ˆã¿JSONã‚’åŸºã«ã€ãƒ•ã‚¡ã‚¤ãƒ«åã®å‰æ–¹ä¸€è‡´ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œç´¢ã—ã€
    æœ€çµ‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆCSVã‚’ä½œæˆã™ã‚‹ã€‚
    """
    # ---- 1. å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ ----
    if not json_path.is_file():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {json_path}", file=sys.stderr)
        sys.exit(1)
    if not dataset_root.is_dir():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_root}", file=sys.stderr)
        sys.exit(1)

    print("âœ… å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯å®Œäº†ã€‚")
    print(f"ğŸ“– JSONå…¥åŠ›: {json_path}")
    print(f"ğŸ’½ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ«ãƒ¼ãƒˆ: {dataset_root}")

    # ---- 2. JSONãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ----
    with open(json_path, "r", encoding="utf-8") as f:
        all_splits_info = json.load(f)

    # ---- 3. å„ã‚»ãƒƒãƒˆï¼ˆtrain, val, testï¼‰ã”ã¨ã«CSVã‚’ä½œæˆ ----
    for split_name, speakers_data in all_splits_info.items():
        print(f"\n======== {split_name.upper()} ã‚»ãƒƒãƒˆã®å‡¦ç†ã‚’é–‹å§‹ ========")

        output_csv_path = os.path.join(dataset_root, F"{split_name}.csv")
        all_rows = []
        missing_files_count = 0

        # JSONã‹ã‚‰å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã‚’ä½œæˆ
        file_list = []
        for _, data in speakers_data.items():
            file_list.extend(data["filenames"])

        # tqdmã‚’ä½¿ã£ã¦é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
        for base_filename in tqdm(file_list, desc=f"Processing {split_name}"):
            row_paths = []

            # 4ç¨®é¡ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            for condition in CONDITION_DIRS:
                # â˜…â˜…â˜… å¤‰æ›´ç‚¹ â˜…â˜…â˜…
                # æ¤œç´¢å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
                search_dir = dataset_root / split_name / condition
                # å‰æ–¹ä¸€è‡´ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                glob_pattern = f"{base_filename}*.wav"

                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
                # .glob()ã¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™ãŸã‚ã€ãƒªã‚¹ãƒˆã«å¤‰æ›
                found_files = list(search_dir.glob(glob_pattern))

                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Œã°æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ã€ãªã‘ã‚Œã°ç©ºæ–‡å­—ã‚’è¿½åŠ 
                if found_files:
                    row_paths.append(str(found_files[0].resolve()))
                else:
                    row_paths.append("")
                    missing_files_count += 1

            all_rows.append(row_paths)

        # ---- 4. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ ----
        if not all_rows:
            print(f"  - '{split_name}' ã‚»ãƒƒãƒˆã«ã¯å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        try:
            with open(output_csv_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(CSV_HEADER)
                writer.writerows(all_rows)

            print(f"âœ… '{output_csv_path}' ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(all_rows)}")
            if missing_files_count > 0:
                print(
                    f"âš ï¸  æ³¨æ„: {missing_files_count} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚CSVå†…ã®ç©ºæ¬„ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )

        except IOError as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: '{output_csv_path}' ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)

    print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="JSONã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å‰æ–¹ä¸€è‡´ã§æ¤œç´¢ã—ã€æœ€çµ‚çš„ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚"
    )
    parser.add_argument(
        "--json_path",
        type=Path,
        default=DEFAULT_JSON_PATH,
        help=f"å…¥åŠ›ã¨ãªã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸ˆã¿JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_JSON_PATH})",
    )
    parser.add_argument(
        "--dataset_root",
        type=Path,
        default=DEFAULT_DATASET_ROOT,
        help=f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_DATASET_ROOT})",
    )

    args = parser.parse_args()

    create_final_csv_list_prefix(json_path=args.json_path, dataset_root=args.dataset_root)
